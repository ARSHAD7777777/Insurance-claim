# Insurance claim prediction
# Introduction:
### This is an Insurance Claim Prediction Data Set. The bigger scope is to account for inaccuracies in the cost of insurance policies. A cautious driver should get a better price compared to a reckless driver. The aim is to build a machine learning model to predict whether an auto insurance policy holder files a claim.

# Dataset description:
### In data, features that belong to similar groupings are tagged as such in the feature names (e.g., ind, reg, car, calc). In addition, feature names include the postfix bin to indicate binary features and cat to indicate categorical features. Features without these designations are either continuous or ordinal. Values of -1 indicate that the feature was missing from the observation. The target columns signifies whether or not a claim was filed for that policy holder.
# Project Analysis
#### The object of this project was to build a Machine learning model to predict whether a auto insurance policy holder files a claim or not. The given dataset had 59 features including 'target' and irrevelant features like 'ID'. The size of the data itself was the first issue to be adressed as it can increase the computational complexity.Firstly,The target variable and the predictor variables were split into seperate variables and then features were organized into seperate lists of continuous, binary,categorical(nominal) and ordinal variables. To check multi-collinearity within continuous and ordinal variables, heatmaps(from seaborn package) of the same were plotted to find out that some of the continuous features shown strong correlation.One among two strongly correlated features were eliminated after considering their significance in the prediction using XGBclassifier.

### The null values in the dataset were assigned the value -1 which were replaced by the standard NaN form and then imputed using the SimpleImputer from scikit learn. The null values in all the categorical variables were replaced with the most frequent(mode) value, while that of continuous variables were replaced with the average(mean value). Afterwards the data was split into training and test data by using the train_test_split function from scikit learn. It was observed that the target variable values were imbalanced in the dataset. Considering that the imbalanced dataset can lead to a biased and inaccurate model, SMOTE package from imblearn API was used for oversampling or synthetic data generation to balance the dataset.

### Since there is no single perfect model for all the prediction tasks, all the classification algorithms including Logistic regression, Random forest classifier, XGBoost classifier, AdaBoost classifier and K-nearest Neighbour classifer, were tried out and the most effective one was chosen by plotting the receiver operator curves(ROC) and evaluating the Area under the curve values(AUC) for each of the models. XGBclassifier was found to be the most effective in prediction and hyperparameter tuning for the same was carried out using the RandomizedSearchCV function from scikit learn. The best model was saved using the joblib package and was tested on the test data to get an accuracy of ~96.35%
